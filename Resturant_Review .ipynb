{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries \n",
    "import numpy as np   ## used for Stastics\n",
    "import pandas as pd ## used for dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the  Resturant Review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('E:/nikhitha/datasets/Restaurant_Reviews.tsv', delimiter = '\\t') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding dimensions of dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting all the rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the nltk package natural language tool kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Language Tool Kit \n",
    "import nltk  # natural language tool kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['Review'] = dataset['Review'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Service was very prompt.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()\n",
    "dataset['Review'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the text data removing other special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(text):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    #review =  [re.sub('[^a-zA-Z]','@', text)]\n",
    "    return review\n",
    "\n",
    "\n",
    "dataset['Review']=dataset['Review'].apply(lambda x : clean(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not tasty and the texture was just nasty \n"
     ]
    }
   ],
   "source": [
    "print(dataset['Review'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the sentences into words called tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer \n",
    "tokenizer = TreebankWordTokenizer() \n",
    "def token(text):\n",
    "    review = tokenizer.tokenize(text)\n",
    "#     review = [tokenizer.tokenize(text)]\n",
    "    return review\n",
    "\n",
    "dataset['Review']=dataset['Review'].apply(lambda x : token(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow', 'Loved', 'this', 'place']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['Review'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing the words removing the suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lematizer(text):\n",
    "    review = [lemmatizer.lemmatize(i) for i in text]\n",
    "    return review\n",
    "    \n",
    "dataset['Review']=dataset['Review'].apply(lambda x : lematizer(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow', 'Loved', 'this', 'place']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['Review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Crust', 'is', 'not', 'good']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['Review'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def stop_word(text):\n",
    "    #stopwords=get_stop_words(\"resources/stopwords.txt\")\n",
    "    review = [i for i in text if i not in stopwords.words('english')]\n",
    "    return review\n",
    "\n",
    "dataset['Review']=dataset['Review'].apply(lambda x : stop_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow', 'Loved', 'place']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['Review'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting words to lower case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# cv = CountVectorizer(lowercase=True,max_df = 0.50, stop_words=stopwords.words('english'))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#from nltk.stem import WordNetLemmatizer \n",
    "#from nltk.tokenize import TreebankWordTokenizer\n",
    "#lemmatizer = WordNetLemmatizer(text)\n",
    "# token = TreebankWordTokenizer() \n",
    "# def token(text):\n",
    "#     review = token.tokenize\n",
    "# #     review = [tokenizer.tokenize(text)]\n",
    "#     return review\n",
    "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "#token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "#dataset['Review'] = dataset['Review'].str.lower()\n",
    "cv = CountVectorizer(lowercase=True)\n",
    "word_count_vector = cv.fit_transform(dataset['Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              ['Wow', 'Loved', 'place']\n",
       "1                                      ['Crust', 'good']\n",
       "2             ['Not', 'tasty', 'texture', 'wa', 'nasty']\n",
       "3      ['Stopped', 'late', 'May', 'bank', 'holiday', ...\n",
       "4      ['The', 'selection', 'menu', 'wa', 'great', 'p...\n",
       "5      ['Now', 'I', 'getting', 'angry', 'I', 'want', ...\n",
       "6                 ['Honeslty', 'taste', 'THAT', 'fresh']\n",
       "7      ['The', 'potato', 'like', 'rubber', 'could', '...\n",
       "8                                ['The', 'fry', 'great']\n",
       "9                                ['A', 'great', 'touch']\n",
       "10                           ['Service', 'wa', 'prompt']\n",
       "11                               ['Would', 'go', 'back']\n",
       "12     ['The', 'cashier', 'care', 'ever', 'I', 'say',...\n",
       "13     ['I', 'tried', 'Cape', 'Cod', 'ravoli', 'chick...\n",
       "14     ['I', 'wa', 'disgusted', 'I', 'wa', 'pretty', ...\n",
       "15     ['I', 'wa', 'shocked', 'sign', 'indicate', 'ca...\n",
       "16                             ['Highly', 'recommended']\n",
       "17       ['Waitress', 'wa', 'little', 'slow', 'service']\n",
       "18     ['This', 'place', 'worth', 'time', 'let', 'alo...\n",
       "19                                              ['like']\n",
       "20                          ['The', 'Burrittos', 'Blah']\n",
       "21                            ['The', 'food', 'amazing']\n",
       "22                           ['Service', 'also', 'cute']\n",
       "23     ['I', 'could', 'care', 'le', 'The', 'interior'...\n",
       "24                                   ['So', 'performed']\n",
       "25     ['That', 'right', 'red', 'velvet', 'cake', 'oh...\n",
       "26        ['They', 'never', 'brought', 'salad', 'asked']\n",
       "27     ['This', 'hole', 'wall', 'ha', 'great', 'Mexic...\n",
       "28     ['Took', 'hour', 'get', 'food', 'table', 'rest...\n",
       "29           ['The', 'worst', 'wa', 'salmon', 'sashimi']\n",
       "                             ...                        \n",
       "970    ['I', 'immediately', 'said', 'I', 'wanted', 't...\n",
       "971                ['The', 'ambiance', 'much', 'better']\n",
       "972    ['Unfortunately', 'set', 'u', 'disapppointment...\n",
       "973                              ['The', 'food', 'good']\n",
       "974    ['Your', 'server', 'suck', 'wait', 'correction...\n",
       "975    ['What', 'happened', 'next', 'wa', 'pretty', '...\n",
       "976    ['bad', 'cause', 'I', 'know', 'family', 'owned...\n",
       "977                            ['Overpriced', 'getting']\n",
       "978         ['I', 'vomited', 'bathroom', 'mid', 'lunch']\n",
       "979    ['I', 'kept', 'looking', 'time', 'soon', 'beco...\n",
       "980    ['I', 'place', 'eat', 'circumstance', 'would',...\n",
       "981    ['We', 'started', 'tuna', 'sashimi', 'wa', 'br...\n",
       "982                            ['Food', 'wa', 'average']\n",
       "983    ['It', 'sure', 'doe', 'beat', 'nacho', 'movie'...\n",
       "984    ['All', 'Ha', 'Long', 'Bay', 'wa', 'bit', 'flop']\n",
       "985    ['The', 'problem', 'I', 'charge', 'sandwich', ...\n",
       "986    ['Shrimp', 'When', 'I', 'unwrapped', 'I', 'liv...\n",
       "987    ['It', 'lacked', 'flavor', 'seemed', 'undercoo...\n",
       "988    ['It', 'really', 'impressive', 'place', 'closed']\n",
       "989    ['I', 'would', 'avoid', 'place', 'staying', 'M...\n",
       "990    ['The', 'refried', 'bean', 'came', 'meal', 'dr...\n",
       "991          ['Spend', 'money', 'time', 'place', 'else']\n",
       "992    ['A', 'lady', 'table', 'next', 'u', 'found', '...\n",
       "993              ['presentation', 'food', 'wa', 'awful']\n",
       "994             ['I', 'tell', 'disappointed', 'I', 'wa']\n",
       "995    ['I', 'think', 'food', 'flavor', 'texture', 'l...\n",
       "996                    ['Appetite', 'instantly', 'gone']\n",
       "997    ['Overall', 'I', 'wa', 'impressed', 'would', '...\n",
       "998    ['The', 'whole', 'experience', 'wa', 'underwhe...\n",
       "999    ['Then', 'I', 'wasted', 'enough', 'life', 'pou...\n",
       "Name: Review, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"Review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_test = pd.read_csv('TrainSA.csv')\n",
    "dataset.Review=dataset.Review.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow',\n",
       " 'loved',\n",
       " 'place',\n",
       " 'crust',\n",
       " 'good',\n",
       " 'not',\n",
       " 'tasty',\n",
       " 'texture',\n",
       " 'wa',\n",
       " 'nasty']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.iloc[:, 1].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into \n",
    "# the Training set and Test set \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)\n",
    "#from sklearn.cross_validation import train_test_split \n",
    "  \n",
    "# experiment with \"test_size\" \n",
    "# to get better results \n",
    "X_train, X_test, y_train, y_test = train_test_split(word_count_vector, y, test_size = 0.25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=501, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "  \n",
    "# n_estimators can be said as number of \n",
    "# trees, experiment with n_estimators \n",
    "# to get better results  \n",
    "model = RandomForestClassifier(n_estimators = 501, \n",
    "                            criterion = 'entropy') \n",
    "                              \n",
    "model.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test) \n",
    "  \n",
    "y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[106  24]\n",
      " [ 35  85]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.82      0.78       130\n",
      "           1       0.78      0.71      0.74       120\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       250\n",
      "   macro avg       0.77      0.76      0.76       250\n",
      "weighted avg       0.77      0.76      0.76       250\n",
      "\n",
      "0.764\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
